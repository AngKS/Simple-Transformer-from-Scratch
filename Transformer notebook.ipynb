{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Transformer from Scratch\n",
        "\n",
        "Reference: ([link text](https://medium.com/towards-data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd))"
      ],
      "metadata": {
        "id": "V_t26ystD9ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C1JYoESg-84x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention"
      ],
      "metadata": {
        "id": "jd-tjBnCEYkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the multi-head attention class for the transformer model\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_heads=4):\n",
        "        \"\"\"\n",
        "        input_dim: Dimensionality of the input.\n",
        "        num_heads: The number of attention heads to split the input into.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        # check if the hidden dimensions are divisible by the number of heads\n",
        "        assert hidden_dim % num_heads == 0, \"Hidden Dimensions must be divisible by number of heads\"\n",
        "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n",
        "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n",
        "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n",
        "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Output layer\n",
        "\n",
        "    def check_sdpa_inputs(self, x):\n",
        "        assert x.size(1) == self.num_heads, f\"Expected size of x to be ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}), got {x.size()}\"\n",
        "        assert x.size(3) == self.hidden_dim // self.num_heads\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, attention_mask=None, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        query: tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n",
        "        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n",
        "        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n",
        "        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n",
        "        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.check_sdpa_inputs(query)\n",
        "        self.check_sdpa_inputs(key)\n",
        "        self.check_sdpa_inputs(value)\n",
        "\n",
        "        d_k = query.size(-1)\n",
        "        target_len, source_len = query.size(-2), key.size(-2)\n",
        "\n",
        "        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n",
        "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        # Creating the attention mask here\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.dim() == 2:\n",
        "                assert attention_mask.size() == (target_len, source_len)\n",
        "                attention_mask = attention_mask.unsqueeze(0)\n",
        "                logits = logits + attention_mask\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Attention mask size {attention_mask.size()}\")\n",
        "\n",
        "        # Creating Key mask here\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, number of heads\n",
        "            logits = logits + key_padding_mask\n",
        "\n",
        "        attention = torch.softmax(logits, dim=-1)\n",
        "        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dims)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "    def split_into_heads(self, x, num_heads):\n",
        "        batch_size, seq_length, hidden_dim = x.size()\n",
        "        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n",
        "\n",
        "        return x.transpose(1, 2) # Final dimension will be (batch_size, num_heads, seq_length, hidden_dim // num_heads)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)\n",
        "\n",
        "    def forward(self, query, key, value, attention_mask=None, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        query : tensor of shape (batch_size, query_sequence_length, hidden_dim)\n",
        "        key : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n",
        "        value : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n",
        "        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n",
        "        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n",
        "\n",
        "        \"\"\"\n",
        "        query = self.Wq(query)\n",
        "        key = self.Wk(key)\n",
        "        value = self.Wv(value)\n",
        "\n",
        "        query = self.split_into_heads(query, self.num_heads)\n",
        "        key = self.split_into_heads(key, self.num_heads)\n",
        "        value = self.split_into_heads(value, self.num_heads)\n",
        "\n",
        "        attention_values, attention_weights = self.scaled_dot_product_attention(\n",
        "            query=query,\n",
        "            key=key,\n",
        "            value=value,\n",
        "            attention_mask=attention_mask,\n",
        "            key_padding_mask=key_padding_mask\n",
        "        )\n",
        "\n",
        "        grouped = self.combine_heads(attention_values)\n",
        "        output = self.Wo(grouped)\n",
        "\n",
        "        self.attention_weights = attention_weights\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "Lt93lfZV_IvG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Class\n",
        "\n",
        "When receiving and treating an input, a transformer has no sense of order as it looks at the sequence as a whole, in opposition to what RNNs do. We therefore need to add a hint of temporal order so that the transformer can learn dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "a2OzVPwLEjjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding part of the Encoder"
      ],
      "metadata": {
        "id": "xIpE-jlnGqLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "QMaAE7AUD4pV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward part of the Encoder"
      ],
      "metadata": {
        "id": "nW7Dd2m3F4rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "VWPO6neiF6eW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder block part of the Encoder"
      ],
      "metadata": {
        "id": "0fFglnAZHz_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self.MHA = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
        "        self.norm1 = nn.LayerNorm(n_dim)\n",
        "        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n",
        "        self.norm2 = nn.LayerNorm(n_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, src_padding_mask=None):\n",
        "        assert x.ndim==3, f\"Expected input to be 3-Dimensional; got {x.ndim}\"\n",
        "        attention_output = self.MHA(x, x, x, key_padding_mask=src_padding_mask)\n",
        "\n",
        "        ff_output = self.ff(x)\n",
        "        output = x + self.norm2(ff_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "l-dyKb4mGiQI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Class"
      ],
      "metadata": {
        "id": "q4-UBLAtIyNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size: int,\n",
        "            n_dim: int,\n",
        "            dropout: float,\n",
        "            n_encoder_blocks: int,\n",
        "            n_heads: int\n",
        "        ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_dim = n_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_dim)\n",
        "        self.positional_encoding = PositionalEncoding(d_model=n_dim, dropout=dropout)\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock(n_dim, dropout, n_heads) for _ in range(n_encoder_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self,x, padding_mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.n_dim)\n",
        "        x = self.positional_encoding(x)\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x=x, src_padding_mask=padding_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6Oij-LpmHwsr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Class"
      ],
      "metadata": {
        "id": "3Qx2t0ulI7Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # The first Multi-Head Attention has a mask to avoid looking at the future\n",
        "        self.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
        "        self.norm1 = nn.LayerNorm(n_dim)\n",
        "\n",
        "        # The second Multi-Head Attention will take inputs from the encoder as key/value inputs\n",
        "        self.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
        "        self.norm2 = nn.LayerNorm(n_dim)\n",
        "\n",
        "        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n",
        "        self.norm3 = nn.LayerNorm(n_dim)\n",
        "\n",
        "    def forward(self, target, memory, target_mask=None, target_padding_mask=None, memory_padding_mask=None):\n",
        "        masked_attention_output = self.self_attention(\n",
        "            query = target,\n",
        "            key = target,\n",
        "            value = target,\n",
        "            attention_mask = target_mask,\n",
        "            key_padding_mask = target_padding_mask\n",
        "        )\n",
        "        x1 = target + self.norm1(masked_attention_output)\n",
        "\n",
        "        cross_attention_output = self.cross_attention(\n",
        "            query = x1,\n",
        "            key = memory,\n",
        "            value = memory,\n",
        "            attention_mask = None,\n",
        "            key_padding_mask = memory_padding_mask\n",
        "        )\n",
        "\n",
        "        ff_output = self.ff(x2)\n",
        "        output = x2 + self.norm3(ff_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "WLk-IBKRIxlI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, n_dim: int, dropout: float, max_seq_len: int, n_decoder_blocks: int, n_heads: int):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = n_dim)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model = n_dim,\n",
        "            dropout = dropout\n",
        "        )\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(n_dim, dropout, n_heads) for _ in range(n_decoder_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, target, memory, target_mask=None, target_padding_mask=None, memory_padding_mask=None):\n",
        "        x = self.embedding(target)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for block in self.decoder_blocks:\n",
        "            x = block(\n",
        "                x,\n",
        "                memory,\n",
        "                target_mask = target_mask,\n",
        "                target_padding_mask = target_padding_mask,\n",
        "                memory_padding_mask= memory_padding_mask\n",
        "            )\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fo1UNIh7cdui"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            print(f\" * {key}={value}\")\n",
        "\n",
        "        self.vocab_size = kwargs.get('vocab_size')\n",
        "        self.model_dim = kwargs.get('model_dim')\n",
        "        self.dropout = kwargs.get('dropout')\n",
        "        self.n_encoder_layers = kwargs.get('n_encoder_layers')\n",
        "        self.n_decoder_layers = kwargs.get('n_decoder_layers')\n",
        "        self.n_heads = kwargs.get('n_heads')\n",
        "        self.batch_size = kwargs.get('batch_size')\n",
        "        self.PAD_IDX = kwargs.get('pad_idx', 0)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            self.vocab_size,\n",
        "            self.model_dim,\n",
        "            self.dropout,\n",
        "            self.n_encoder_layers,\n",
        "            self.n_heads\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            self.vocab_size,\n",
        "            self.model_dim,\n",
        "            self.dropout,\n",
        "            self.n_decoder_layers,\n",
        "            self.n_heads\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(self.model_dim, self.vocab_size)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(size: int):\n",
        "        # Generate a Triangular (size, size) mask\n",
        "\n",
        "        mask = (1 - torch.triu(torch.ones(size, size), diagonal = 1)).bool()\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input\n",
        "            x: (B, S) with elements in (0, C) where C is num_classes\n",
        "        Output\n",
        "            (B, S, E) embedding\n",
        "        \"\"\"\n",
        "\n",
        "        mask = (x == self.PAD_IDX).float()\n",
        "        encoder_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "        # (B, S, E)\n",
        "        encoder_output = self.encoder(\n",
        "            x,\n",
        "            padding_mask = encoder_padding_mask\n",
        "        )\n",
        "\n",
        "        return encoder_output, encoder_padding_mask\n",
        "\n",
        "\n",
        "    def decode(\n",
        "            self,\n",
        "            target: torch.Tensor,\n",
        "            memory: torch.Tensor,\n",
        "            memory_padding_mask=None\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        B = Batch size\n",
        "        S = Source sequence length\n",
        "        L = Target sequence length\n",
        "        E = Model dimension\n",
        "\n",
        "        Input\n",
        "            encoded_x: (B, S, E)\n",
        "            y: (B, L) with elements in (0, C) where C is num_classes\n",
        "        Output\n",
        "            (B, L, C) logits\n",
        "        \"\"\"\n",
        "\n",
        "        mask = (target == self.PAD_IDX).float()\n",
        "        target_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "        decoder_output = self.decoder(\n",
        "            target=target,\n",
        "            memory=memory,\n",
        "            tgt_mask=self.generate_square_subsequent_mask(target.size(1)),\n",
        "            target_padding_mask=target_padding_mask,\n",
        "            memory_padding_mask=memory_padding_mask,\n",
        "\n",
        "        )\n",
        "        output = self.fc(decoder_output)  # shape (B, L, C)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        y: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input\n",
        "            x: (B, Sx) with elements in (0, C) where C is num_classes\n",
        "            y: (B, Sy) with elements in (0, C) where C is num_classes\n",
        "        Output\n",
        "            (B, L, C) logits\n",
        "        \"\"\"\n",
        "\n",
        "        # Encoder output shape (B, S, E)\n",
        "        encoder_output, encoder_padding_mask = self.encode(x)\n",
        "\n",
        "        # Decoder output shape (B, L, C)\n",
        "        decoder_output = self.decode(\n",
        "            target=y,\n",
        "            memory=encoder_output,\n",
        "            memory_padding_mask=encoder_padding_mask,\n",
        "        )\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        sos_idx: int=1,\n",
        "        eos_idx: int=2,\n",
        "        max_length: int=None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Method to use at inference time. Predict y from x one token at a time. This method is greedy\n",
        "        decoding. Beam search can be used instead for a potential accuracy boost.\n",
        "\n",
        "        Input\n",
        "            x: str\n",
        "        Output\n",
        "            (B, L, C) logits\n",
        "        \"\"\"\n",
        "\n",
        "        # Pad the tokens with beginning and end of sentence tokens\n",
        "        x = torch.cat([\n",
        "            torch.tensor([sos_idx]),\n",
        "            x,\n",
        "            torch.tensor([eos_idx])]\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        encoder_output, mask = self.transformer.encode(x) # (B, S, E)\n",
        "\n",
        "        if not max_length:\n",
        "            max_length = x.size(1)\n",
        "\n",
        "        outputs = torch.ones((x.size()[0], max_length)).type_as(x).long() * sos_idx\n",
        "        for step in range(1, max_length):\n",
        "            y = outputs[:, :step]\n",
        "            probs = self.transformer.decode(y, encoder_output)\n",
        "            output = torch.argmax(probs, dim=-1)\n",
        "\n",
        "            # Uncomment if you want to see step by step predicitons\n",
        "            # print(f\"Knowing {y} we output {output[:, -1]}\")\n",
        "\n",
        "            if output[:, -1].detach().numpy() in (eos_idx, sos_idx):\n",
        "                break\n",
        "            outputs[:, step] = output[:, -1]\n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "O8SW8o45dkoH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Toy dataset"
      ],
      "metadata": {
        "id": "hEK4QFY0fl7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def generate_random_string():\n",
        "    len = np.random.randint(10, 20)\n",
        "    return \"\".join([chr(x) for x in np.random.randint(97, 97+26, len)])\n",
        "\n",
        "class ReverseDataset(Dataset):\n",
        "    def __init__(self, n_samples, pad_idx, sos_idx, eos_idx):\n",
        "        super(ReverseDataset, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.sos_idx = sos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.values = [generate_random_string() for _ in range(n_samples)]\n",
        "        self.labels = [x[::-1] for x in self.values]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.values)  # number of samples in the dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.text_transform(self.values[index].rstrip(\"\\n\")), \\\n",
        "            self.text_transform(self.labels[index].rstrip(\"\\n\"))\n",
        "\n",
        "    def text_transform(self, x):\n",
        "        return torch.tensor([self.sos_idx] + [ord(z)-97+3 for z in x] + [self.eos_idx])"
      ],
      "metadata": {
        "id": "UHvABmoNfJ-i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training and Evaluation Steps"
      ],
      "metadata": {
        "id": "v1DRMBOwfqe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "PAD_IDX = 0\n",
        "SOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "\n",
        "def train(model, optimizer, loader, loss_fn, epoch):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    history_loss = []\n",
        "    history_acc = []\n",
        "\n",
        "    with tqdm(loader, position=0, leave=True) as tepoch:\n",
        "        for x, y in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x, y[:, :-1])\n",
        "            loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses += loss.item()\n",
        "\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            masked_pred = preds * (y[:, 1:]!=PAD_IDX)\n",
        "            accuracy = (masked_pred == y[:, 1:]).float().mean()\n",
        "            acc += accuracy.item()\n",
        "\n",
        "            history_loss.append(loss.item())\n",
        "            history_acc.append(accuracy.item())\n",
        "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy.item())\n",
        "\n",
        "    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "E8RoFQrHfsdj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    history_loss = []\n",
        "    history_acc = []\n",
        "\n",
        "    for x, y in tqdm(loader, position=0, leave=True):\n",
        "\n",
        "        logits = model(x, y[:, :-1])\n",
        "        loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        masked_pred = preds * (y[:, 1:]!=PAD_IDX)\n",
        "        accuracy = (masked_pred == y[:, 1:]).float().mean()\n",
        "        acc += accuracy.item()\n",
        "\n",
        "        history_loss.append(loss.item())\n",
        "        history_acc.append(accuracy.item())\n",
        "\n",
        "    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc"
      ],
      "metadata": {
        "id": "iYN8Jh-yfvmx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train!"
      ],
      "metadata": {
        "id": "DG65f8LYfzkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    This function pads inputs with PAD_IDX to have batches of equal length\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "xUM_BwEbf1pT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "args = {\n",
        "    'vocab_size': 128,\n",
        "    'model_dim': 128,\n",
        "    'dropout': 0.1,\n",
        "    'n_encoder_layers': 1,\n",
        "    'n_decoder_layers': 1,\n",
        "    'n_heads': 4\n",
        "}\n",
        "\n",
        "# Define model here\n",
        "model = SimpleTransformer(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "SI0RUMuqf30o",
        "outputId": "1c0f6d4a-774c-45dc-803b-50e854d33678"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * vocab_size=128\n",
            " * model_dim=128\n",
            " * dropout=0.1\n",
            " * n_encoder_layers=1\n",
            " * n_decoder_layers=1\n",
            " * n_heads=4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Decoder.__init__() missing 1 required positional argument: 'n_heads'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-182596ceeea1>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Define model here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-794010347807>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         self.decoder = Decoder(\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Decoder.__init__() missing 1 required positional argument: 'n_heads'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate datasets\n",
        "train_iter = ReverseDataset(50000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX)\n",
        "eval_iter = ReverseDataset(10000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX)\n",
        "dataloader_train = DataLoader(train_iter, batch_size=256, collate_fn=collate_fn)\n",
        "dataloader_val = DataLoader(eval_iter, batch_size=256, collate_fn=collate_fn)\n",
        "\n",
        "# During debugging, we ensure sources and targets are indeed reversed\n",
        "# s, t = next(iter(dataloader_train))\n",
        "# print(s[:4, ...])\n",
        "# print(t[:4, ...])\n",
        "# print(s.size())\n",
        "\n",
        "# Initialize model parameters\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Define loss function : we ignore logits which are padding tokens\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Save history to dictionnary\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'eval_loss': [],\n",
        "    'train_acc': [],\n",
        "    'eval_acc': []\n",
        "}"
      ],
      "metadata": {
        "id": "5ReRaVP4f8F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop\n",
        "for epoch in range(1, 4):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc, hist_loss, hist_acc = train(model, optimizer, dataloader_train, loss_fn, epoch)\n",
        "    history['train_loss'] += hist_loss\n",
        "    history['train_acc'] += hist_acc\n",
        "    end_time = time.time()\n",
        "    val_loss, val_acc, hist_loss, hist_acc = evaluate(model, dataloader_val, loss_fn)\n",
        "    history['eval_loss'] += hist_loss\n",
        "    history['eval_acc'] += hist_acc\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train acc: {train_acc:.3f}, Val loss: {val_loss:.3f}, Val acc: {val_acc:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "metadata": {
        "id": "dueQPJuvfv-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing our model!"
      ],
      "metadata": {
        "id": "xCaDNHtGgBTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, transformer):\n",
        "        super(Translator, self).__init__()\n",
        "        self.transformer = transformer\n",
        "\n",
        "    @staticmethod\n",
        "    def str_to_tokens(s):\n",
        "        return [ord(z)-97+3 for z in s]\n",
        "\n",
        "    @staticmethod\n",
        "    def tokens_to_str(tokens):\n",
        "        return \"\".join([chr(x+94) for x in tokens])\n",
        "\n",
        "    def __call__(self, sentence, max_length=None, pad=False):\n",
        "\n",
        "        x = torch.tensor(self.str_to_tokens(sentence))\n",
        "\n",
        "        outputs = self.transformer.predict(sentence)\n",
        "\n",
        "        return self.tokens_to_str(outputs[0])"
      ],
      "metadata": {
        "id": "JbPt-ULRgChR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(model)\n",
        "\n",
        "sentence = \"Hello World!\"\n",
        "print(sentence)\n",
        "\n",
        "output = translator(sentence)\n",
        "print(output: output)"
      ],
      "metadata": {
        "id": "Bk9tm-UzgEIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}